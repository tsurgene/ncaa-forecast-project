{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "deb1bda2-32be-40ad-a562-876b96cea610",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/code/rustyb/paris-madness-2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dff55e8-8b8a-4135-9f83-4750f07b4d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import log_loss\n",
    "from scipy.interpolate import UnivariateSpline\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "\n",
    "pd.set_option(\"display.max_column\", 999)\n",
    "print(os.listdir(\"../input\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f2f1da-2f4d-41c9-a3ca-f87a562badeb",
   "metadata": {},
   "source": [
    "### **Load the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d906c8-489b-410d-b368-d7859b067122",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '/kaggle/input/march-machine-learning-mania-2023/'\n",
    "\n",
    "tourney_results = pd.concat([\n",
    "    pd.read_csv(DATA_PATH + \"MNCAATourneyDetailedResults.csv\"),\n",
    "    pd.read_csv(DATA_PATH + \"WNCAATourneyDetailedResults.csv\"),\n",
    "], ignore_index=True)\n",
    "\n",
    "seeds = pd.concat([\n",
    "    pd.read_csv(DATA_PATH + \"MNCAATourneySeeds.csv\"),\n",
    "    pd.read_csv(DATA_PATH + \"WNCAATourneySeeds.csv\"),\n",
    "], ignore_index=True)\n",
    "\n",
    "regular_results = pd.concat([\n",
    "    pd.read_csv(DATA_PATH + \"MRegularSeasonDetailedResults.csv\"),\n",
    "    pd.read_csv(DATA_PATH + \"WRegularSeasonDetailedResults.csv\"),\n",
    "], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30aa4a1-3d43-45ee-a682-878b4fd23615",
   "metadata": {},
   "outputs": [],
   "source": [
    "tourney_results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232c0379-e42f-44ad-8a50-5d07cfc151b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba588a22-532b-4f56-b669-5792a4ed7831",
   "metadata": {},
   "outputs": [],
   "source": [
    "regular_results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e0279a-e714-419a-9eb9-551ec2b23527",
   "metadata": {},
   "outputs": [],
   "source": [
    "regular_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4fd06b-2794-41ab-806c-75528b8a7095",
   "metadata": {},
   "outputs": [],
   "source": [
    "regular_results.groupby('Season').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5ea4ed-080b-41b1-bf77-c6a033777ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tourney_results.groupby('Season').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3726ed0-a3a4-46c5-bcce-2f71886d3b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7a836d-568c-43c7-a2a7-26bfa7f332fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tourney_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c751114c-a469-43bc-b6c6-1e26e7758bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "all(regular_results.columns == tourney_results.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b580a994-9975-4cec-b073-6d6357cfa6ed",
   "metadata": {},
   "source": [
    "### **Data preparation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffe7fe6-e77b-4a87-b7d0-666c917d12ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "regular_results.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89853a1d-8aa2-45bd-a968-ecc08b2b36fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "regular_results_swap = regular_results[[\n",
    "    'Season', 'DayNum', 'LTeamID', 'LScore', 'WTeamID', 'WScore', 'WLoc', 'NumOT', \n",
    "    'LFGM', 'LFGA', 'LFGM3', 'LFGA3', 'LFTM', 'LFTA', 'LOR', 'LDR', 'LAst', 'LTO', 'LStl', 'LBlk', 'LPF', \n",
    "    'WFGM', 'WFGA', 'WFGM3', 'WFGA3', 'WFTM', 'WFTA', 'WOR', 'WDR', 'WAst', 'WTO', 'WStl', 'WBlk', 'WPF']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff5aca5-40e4-4452-9ac7-e69a8646f864",
   "metadata": {},
   "outputs": [],
   "source": [
    "regular_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2ede26-6f9c-4139-b347-c05074c9c8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "regular_results_swap.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1003c1aa-2736-469c-a657-349013dec9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "regular_results_swap.loc[regular_results['WLoc'] == 'H', 'WLoc'] = 'A'\n",
    "regular_results_swap.loc[regular_results['WLoc'] == 'A', 'WLoc'] = 'H'\n",
    "regular_results.columns.values[6] = 'location'\n",
    "regular_results_swap.columns.values[6] = 'location'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0116a09-8bcc-4fbe-bb8c-a86a5ee7c7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "regular_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4fbaa8-63e9-44a0-9970-61266a426b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "regular_results_swap.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486bb59b-60bc-4145-8989-16adf67f9fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "regular_results.columns = [x.replace('W','T1_').replace('L','T2_') for x in list(regular_results.columns)]\n",
    "regular_results_swap.columns = [x.replace('L','T1_').replace('W','T2_') for x in list(regular_results.columns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a7c95e-c1ce-4e03-84cc-518cfd4c2022",
   "metadata": {},
   "outputs": [],
   "source": [
    "regular_results.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734bf66a-b6d4-42c7-b307-a0cfc626dc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "regular_results_swap.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db3a6e7-f89b-4b35-9f7f-3151bf10a648",
   "metadata": {},
   "outputs": [],
   "source": [
    "regular_data = pd.concat([regular_results, regular_results_swap]).sort_index().reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9de147-7c9d-4be5-92d1-3e033275776b",
   "metadata": {},
   "outputs": [],
   "source": [
    "regular_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692a0846-fe15-47bb-a53a-f31d67e898b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "tourney_results = pd.read_csv('../input/WNCAATourneyDetailedResults.csv')\n",
    "seeds = pd.read_csv('../input/WNCAATourneySeeds.csv')\n",
    "regular_results = pd.read_csv('../input/WRegularSeasonDetailedResults.csv')\n",
    "\"\"\"\n",
    "\n",
    "tourney_results = pd.concat([\n",
    "    pd.read_csv(DATA_PATH + \"MNCAATourneyDetailedResults.csv\"),\n",
    "    pd.read_csv(DATA_PATH + \"WNCAATourneyDetailedResults.csv\"),\n",
    "], ignore_index=True)\n",
    "\n",
    "seeds = pd.concat([\n",
    "    pd.read_csv(DATA_PATH + \"MNCAATourneySeeds.csv\"),\n",
    "    pd.read_csv(DATA_PATH + \"WNCAATourneySeeds.csv\"),\n",
    "], ignore_index=True)\n",
    "\n",
    "regular_results = pd.concat([\n",
    "    pd.read_csv(DATA_PATH + \"MRegularSeasonDetailedResults.csv\"),\n",
    "    pd.read_csv(DATA_PATH + \"WRegularSeasonDetailedResults.csv\"),\n",
    "], ignore_index=True)\n",
    "\n",
    "\n",
    "def prepare_data(df):\n",
    "    dfswap = df[['Season', 'DayNum', 'LTeamID', 'LScore', 'WTeamID', 'WScore', 'WLoc', 'NumOT', \n",
    "    'LFGM', 'LFGA', 'LFGM3', 'LFGA3', 'LFTM', 'LFTA', 'LOR', 'LDR', 'LAst', 'LTO', 'LStl', 'LBlk', 'LPF', \n",
    "    'WFGM', 'WFGA', 'WFGM3', 'WFGA3', 'WFTM', 'WFTA', 'WOR', 'WDR', 'WAst', 'WTO', 'WStl', 'WBlk', 'WPF']]\n",
    "\n",
    "    dfswap.loc[df['WLoc'] == 'H', 'WLoc'] = 'A'\n",
    "    dfswap.loc[df['WLoc'] == 'A', 'WLoc'] = 'H'\n",
    "    df.columns.values[6] = 'location'\n",
    "    dfswap.columns.values[6] = 'location'    \n",
    "      \n",
    "    df.columns = [x.replace('W','T1_').replace('L','T2_') for x in list(df.columns)]\n",
    "    dfswap.columns = [x.replace('L','T1_').replace('W','T2_') for x in list(dfswap.columns)]\n",
    "\n",
    "    output = pd.concat([df, dfswap]).reset_index(drop=True)\n",
    "    output.loc[output.location=='N','location'] = '0'\n",
    "    output.loc[output.location=='H','location'] = '1'\n",
    "    output.loc[output.location=='A','location'] = '-1'\n",
    "    output.location = output.location.astype(int)\n",
    "    \n",
    "    output['PointDiff'] = output['T1_Score'] - output['T2_Score']\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb432563-ae74-4423-8bb4-56fdab9324d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "regular_data = prepare_data(regular_results)\n",
    "tourney_data = prepare_data(tourney_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e764d790-62f5-4823-9b7f-9a1fda54be8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "regular_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c36a2d-e6a1-42f7-8227-648df98044f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tourney_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b5dba5-0c26-429d-801c-0b8b3cab94c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "regular_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e86166-66aa-48aa-97ad-d2cec6e5b29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tourney_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64175dc-356c-40b7-a287-7e75d913e4eb",
   "metadata": {},
   "source": [
    "### **Feature engineering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d97f7f9-8539-43d1-9fa2-9a7f6207c1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "tourney_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c96839e-21ae-44b3-9f1f-797f2e925c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "boxscore_cols = ['T1_Score', 'T2_Score', \n",
    "        'T1_FGM', 'T1_FGA', 'T1_FGM3', 'T1_FGA3', 'T1_FTM', 'T1_FTA', 'T1_OR', 'T1_DR', 'T1_Ast', 'T1_TO', 'T1_Stl', 'T1_Blk', 'T1_PF', \n",
    "        'T2_FGM', 'T2_FGA', 'T2_FGM3', 'T2_FGA3', 'T2_FTM', 'T2_FTA', 'T2_OR', 'T2_DR', 'T2_Ast', 'T2_TO', 'T2_Stl', 'T2_Blk', 'T2_PF', \n",
    "        'PointDiff']\n",
    "\n",
    "boxscore_cols = [\n",
    "        'T1_FGM', 'T1_FGA', 'T1_FGM3', 'T1_FGA3', 'T1_OR', 'T1_Ast', 'T1_TO', 'T1_Stl', 'T1_PF', \n",
    "        'T2_FGM', 'T2_FGA', 'T2_FGM3', 'T2_FGA3', 'T2_OR', 'T2_Ast', 'T2_TO', 'T2_Stl', 'T2_Blk',  \n",
    "        'PointDiff']\n",
    "\n",
    "funcs = [np.mean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa51828-7ad5-4296-87d7-9e08197feb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "season_statistics = regular_data.groupby([\"Season\", 'T1_TeamID'])[boxscore_cols].agg(funcs)\n",
    "season_statistics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68f773b-aed4-4f50-b9ca-5d470aec2214",
   "metadata": {},
   "outputs": [],
   "source": [
    "season_statistics = regular_data.groupby([\"Season\", 'T1_TeamID'])[boxscore_cols].agg(funcs).reset_index()\n",
    "season_statistics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89234c1-f817-42a4-8d40-5aa732735b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "season_statistics.columns = [''.join(col).strip() for col in season_statistics.columns.values]\n",
    "season_statistics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d58b5d1-e1a7-4169-9aaa-7d05e60510c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "season_statistics_T1 = season_statistics.copy()\n",
    "season_statistics_T2 = season_statistics.copy()\n",
    "\n",
    "season_statistics_T1.columns = [\"T1_\" + x.replace(\"T1_\",\"\").replace(\"T2_\",\"opponent_\") for x in list(season_statistics_T1.columns)]\n",
    "season_statistics_T2.columns = [\"T2_\" + x.replace(\"T1_\",\"\").replace(\"T2_\",\"opponent_\") for x in list(season_statistics_T2.columns)]\n",
    "season_statistics_T1.columns.values[0] = \"Season\"\n",
    "season_statistics_T2.columns.values[0] = \"Season\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9898b50-7461-48dc-b101-38eed1062e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "season_statistics_T1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ac52e2-7d51-41e5-bdb6-6266ce91902b",
   "metadata": {},
   "outputs": [],
   "source": [
    "season_statistics_T2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b37eb4c-d85f-44df-b61a-94371b47d56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tourney_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a540cca2-2ee4-46b5-b59e-0bdf1152e513",
   "metadata": {},
   "outputs": [],
   "source": [
    "tourney_data = tourney_data[['Season', 'DayNum', 'T1_TeamID', 'T1_Score', 'T2_TeamID' ,'T2_Score']]\n",
    "tourney_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515c7bd2-62bd-42fb-8f73-276ef3b4dc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tourney_data = pd.merge(tourney_data, season_statistics_T1, on = ['Season', 'T1_TeamID'], how = 'left')\n",
    "tourney_data = pd.merge(tourney_data, season_statistics_T2, on = ['Season', 'T2_TeamID'], how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c439e7e2-9d50-41a3-ac66-626145d7591a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tourney_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24da724-2c70-456f-b631-bef3a17ee784",
   "metadata": {},
   "outputs": [],
   "source": [
    "last14days_stats_T1 = regular_data.loc[regular_data.DayNum>118].reset_index(drop=True)\n",
    "last14days_stats_T1['win'] = np.where(last14days_stats_T1['PointDiff']>0,1,0)\n",
    "last14days_stats_T1 = last14days_stats_T1.groupby(['Season','T1_TeamID'])['win'].mean().reset_index(name='T1_win_ratio_14d')\n",
    "\n",
    "last14days_stats_T2 = regular_data.loc[regular_data.DayNum>118].reset_index(drop=True)\n",
    "last14days_stats_T2['win'] = np.where(last14days_stats_T2['PointDiff']<0,1,0)\n",
    "last14days_stats_T2 = last14days_stats_T2.groupby(['Season','T2_TeamID'])['win'].mean().reset_index(name='T2_win_ratio_14d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881f6e80-0b63-477e-8dfe-b6936bdbd812",
   "metadata": {},
   "outputs": [],
   "source": [
    "tourney_data = pd.merge(tourney_data, last14days_stats_T1, on = ['Season', 'T1_TeamID'], how = 'left')\n",
    "tourney_data = pd.merge(tourney_data, last14days_stats_T2, on = ['Season', 'T2_TeamID'], how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6caa04e-b39e-4761-beed-c80bb795853c",
   "metadata": {},
   "outputs": [],
   "source": [
    "regular_season_effects = regular_data[['Season','T1_TeamID','T2_TeamID','PointDiff']].copy()\n",
    "regular_season_effects['T1_TeamID'] = regular_season_effects['T1_TeamID'].astype(str)\n",
    "regular_season_effects['T2_TeamID'] = regular_season_effects['T2_TeamID'].astype(str)\n",
    "regular_season_effects['win'] = np.where(regular_season_effects['PointDiff']>0,1,0)\n",
    "march_madness = pd.merge(seeds[['Season','TeamID']],seeds[['Season','TeamID']],on='Season')\n",
    "march_madness.columns = ['Season', 'T1_TeamID', 'T2_TeamID']\n",
    "march_madness.T1_TeamID = march_madness.T1_TeamID.astype(str)\n",
    "march_madness.T2_TeamID = march_madness.T2_TeamID.astype(str)\n",
    "regular_season_effects = pd.merge(regular_season_effects, march_madness, on = ['Season','T1_TeamID','T2_TeamID'])\n",
    "regular_season_effects.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8ab4e5-614f-41a1-b5ec-2dedcdd9d417",
   "metadata": {},
   "outputs": [],
   "source": [
    "regular_season_effects.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb95b29-ad11-4de5-a052-302dbae05cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def team_quality(season):\n",
    "    formula = 'win~-1+T1_TeamID+T2_TeamID'\n",
    "    glm = sm.GLM.from_formula(formula=formula, \n",
    "                              data=regular_season_effects.loc[regular_season_effects.Season==season,:], \n",
    "                              family=sm.families.Binomial()).fit()\n",
    "    \n",
    "    quality = pd.DataFrame(glm.params).reset_index()\n",
    "    quality.columns = ['TeamID','quality']\n",
    "    quality['Season'] = season\n",
    "    #quality['quality'] = np.exp(quality['quality'])\n",
    "    quality = quality.loc[quality.TeamID.str.contains('T1_')].reset_index(drop=True)\n",
    "    quality['TeamID'] = quality['TeamID'].apply(lambda x: x[10:14]).astype(int)\n",
    "    return quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69310fe5-670b-4033-b83d-fa91fb48743d",
   "metadata": {},
   "outputs": [],
   "source": [
    "formula = 'win~-1+T1_TeamID+T2_TeamID'\n",
    "glm = sm.GLM.from_formula(formula=formula, \n",
    "                          data=regular_season_effects.loc[regular_season_effects.Season==2010,:], \n",
    "                          family=sm.families.Binomial()).fit()\n",
    "\n",
    "quality = pd.DataFrame(glm.params).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68abaa35-ca97-4509-b62f-f405c63804a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "quality.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65eedf83-b307-4cf7-a996-a53f1f59bba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85509d9e-0988-48f8-b8fe-404043c4ebab",
   "metadata": {},
   "outputs": [],
   "source": [
    "glm_quality = pd.concat([team_quality(2010),\n",
    "                         team_quality(2011),\n",
    "                         team_quality(2012),\n",
    "                         team_quality(2013),\n",
    "                         team_quality(2014),\n",
    "                         team_quality(2015),\n",
    "                         team_quality(2016),\n",
    "                         team_quality(2017),\n",
    "                         team_quality(2018),\n",
    "                         team_quality(2019),\n",
    "                         ##team_quality(2020),\n",
    "                         team_quality(2021),\n",
    "                         team_quality(2022),\n",
    "                         team_quality(2023)\n",
    "                         ]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b55220-bc63-4b07-b0ac-ff80d53968bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "glm_quality.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e317b9-5f21-4d24-bf9b-4dc1ee02db1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "glm_quality_T1 = glm_quality.copy()\n",
    "glm_quality_T2 = glm_quality.copy()\n",
    "glm_quality_T1.columns = ['T1_TeamID','T1_quality','Season']\n",
    "glm_quality_T2.columns = ['T2_TeamID','T2_quality','Season']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f644bd-6d6b-4b15-99b6-248f92739452",
   "metadata": {},
   "outputs": [],
   "source": [
    "glm_quality_T2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8b7633-e6e0-4811-9e67-cc2e1c549de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tourney_data = pd.merge(tourney_data, glm_quality_T1, on = ['Season', 'T1_TeamID'], how = 'left')\n",
    "tourney_data = pd.merge(tourney_data, glm_quality_T2, on = ['Season', 'T2_TeamID'], how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016bc0b5-9980-441f-9acf-0f3edcc9f867",
   "metadata": {},
   "outputs": [],
   "source": [
    "tourney_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc948cdd-dd2d-4342-996a-00a35d81c82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbbe069-bcca-4a1c-b41f-ad44d231ace1",
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds['seed'] = seeds['Seed'].apply(lambda x: int(x[1:3]))\n",
    "seeds.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226b469d-0444-47ed-8a9a-f7e723c4a2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds_T1 = seeds[['Season','TeamID','seed']].copy()\n",
    "seeds_T2 = seeds[['Season','TeamID','seed']].copy()\n",
    "seeds_T1.columns = ['Season','T1_TeamID','T1_seed']\n",
    "seeds_T2.columns = ['Season','T2_TeamID','T2_seed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fd1ddb-706d-47a9-ae03-6b2f0fbf1d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "tourney_data = pd.merge(tourney_data, seeds_T1, on = ['Season', 'T1_TeamID'], how = 'left')\n",
    "tourney_data = pd.merge(tourney_data, seeds_T2, on = ['Season', 'T2_TeamID'], how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19788033-53c8-4087-b0bc-9b3c13a44c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tourney_data[\"Seed_diff\"] = tourney_data[\"T1_seed\"] - tourney_data[\"T2_seed\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a255f7f2-ee85-4557-879c-e1633d4cc90e",
   "metadata": {},
   "source": [
    "### **Time to build some models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381941dc-5497-4c02-b139-474f5b07b702",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tourney_data['T1_Score'] - tourney_data['T2_Score']\n",
    "y.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1e7e5f-86ab-4d9c-a2cc-242199cffdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list(season_statistics_T1.columns[2:999]) + \\\n",
    "    list(season_statistics_T2.columns[2:999]) + \\\n",
    "    list(seeds_T1.columns[2:999]) + \\\n",
    "    list(seeds_T2.columns[2:999]) + \\\n",
    "    list(last14days_stats_T1.columns[2:999]) + \\\n",
    "    list(last14days_stats_T2.columns[2:999]) + \\\n",
    "    [\"Seed_diff\"] + [\"T1_quality\",\"T2_quality\"]\n",
    "\n",
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e81f3fc-86df-4295-a431-a8985354e9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tourney_data[features].values\n",
    "dtrain = xgb.DMatrix(X, label = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f9076f-7b49-4f79-9108-6bf27c1ccda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cauchyobj(preds, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    c = 5000 \n",
    "    x =  preds-labels    \n",
    "    grad = x / (x**2/c**2+1)\n",
    "    hess = -c**2*(x**2-c**2)/(x**2+c**2)**2\n",
    "    return grad, hess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7730ac49-4d6c-45fd-9901-8b0c57ad8f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {} \n",
    "#param['objective'] = 'reg:linear'\n",
    "param['eval_metric'] =  'mae'\n",
    "param['booster'] = 'gbtree'\n",
    "param['eta'] = 0.05 #change to ~0.02 for final run\n",
    "param['subsample'] = 0.35\n",
    "param['colsample_bytree'] = 0.7\n",
    "param['num_parallel_tree'] = 3 #recommend 10\n",
    "param['min_child_weight'] = 40\n",
    "param['gamma'] = 10\n",
    "param['max_depth'] =  3\n",
    "param['silent'] = 1\n",
    "\n",
    "print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be339b42-1263-4466-80d0-05cc9028c58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_cv = []\n",
    "repeat_cv = 3 # recommend 10\n",
    "\n",
    "for i in range(repeat_cv): \n",
    "    print(f\"Fold repeater {i}\")\n",
    "    xgb_cv.append(\n",
    "        xgb.cv(\n",
    "          params = param,\n",
    "          dtrain = dtrain,\n",
    "          obj = cauchyobj,\n",
    "          num_boost_round = 3000,\n",
    "          folds = KFold(n_splits = 5, shuffle = True, random_state = i),\n",
    "          early_stopping_rounds = 25,\n",
    "          verbose_eval = 50\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f8612d-3837-4c03-83a1-2dab184960af",
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration_counts = [np.argmin(x['test-mae-mean'].values) for x in xgb_cv]\n",
    "val_mae = [np.min(x['test-mae-mean'].values) for x in xgb_cv]\n",
    "iteration_counts, val_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffa8572-3445-4385-8dd8-499537711fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "oof_preds = []\n",
    "for i in range(repeat_cv):\n",
    "    print(f\"Fold repeater {i}\")\n",
    "    preds = y.copy()\n",
    "    kfold = KFold(n_splits = 5, shuffle = True, random_state = i)    \n",
    "    for train_index, val_index in kfold.split(X,y):\n",
    "        dtrain_i = xgb.DMatrix(X[train_index], label = y[train_index])\n",
    "        dval_i = xgb.DMatrix(X[val_index], label = y[val_index])  \n",
    "        model = xgb.train(\n",
    "              params = param,\n",
    "              dtrain = dtrain_i,\n",
    "              num_boost_round = iteration_counts[i],\n",
    "              verbose_eval = 50\n",
    "        )\n",
    "        preds[val_index] = model.predict(dval_i)\n",
    "    oof_preds.append(np.clip(preds,-30,30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f962c7c-3cdc-4f74-b8f5-d4f754b1a42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df = pd.DataFrame({\"pred\":oof_preds[0], \"label\":np.where(y>0,1,0)})\n",
    "plot_df[\"pred_int\"] = plot_df[\"pred\"].astype(int)\n",
    "plot_df = plot_df.groupby('pred_int')['label'].mean().reset_index(name='average_win_pct')\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(plot_df.pred_int,plot_df.average_win_pct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc3fb96-d34c-4ba6-acb2-891cd2823344",
   "metadata": {},
   "outputs": [],
   "source": [
    "spline_model = []\n",
    "\n",
    "for i in range(repeat_cv):\n",
    "    dat = list(zip(oof_preds[i],np.where(y>0,1,0)))\n",
    "    dat = sorted(dat, key = lambda x: x[0])\n",
    "    datdict = {}\n",
    "    for k in range(len(dat)):\n",
    "        datdict[dat[k][0]]= dat[k][1]\n",
    "        \n",
    "    spline_model.append(UnivariateSpline(list(datdict.keys()), list(datdict.values())))\n",
    "    spline_fit = spline_model[i](oof_preds[i])\n",
    "    \n",
    "    print(f\"logloss of cvsplit {i}: {log_loss(np.where(y>0,1,0),spline_fit)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1819464-fde7-40b7-a3f8-24d316c7adf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df = pd.DataFrame({\"pred\":oof_preds[0], \"label\":np.where(y>0,1,0), \"spline\":spline_model[0](oof_preds[0])})\n",
    "plot_df[\"pred_int\"] = (plot_df[\"pred\"]).astype(int)\n",
    "plot_df = plot_df.groupby('pred_int')['spline','label'].mean().reset_index()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(plot_df.pred_int,plot_df.spline)\n",
    "plt.plot(plot_df.pred_int,plot_df.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b718f945-4b77-4613-b268-a8bc0336cafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "spline_model = []\n",
    "\n",
    "for i in range(repeat_cv):\n",
    "    dat = list(zip(oof_preds[i],np.where(y>0,1,0)))\n",
    "    dat = sorted(dat, key = lambda x: x[0])\n",
    "    datdict = {}\n",
    "    for k in range(len(dat)):\n",
    "        datdict[dat[k][0]]= dat[k][1]\n",
    "    spline_model.append(UnivariateSpline(list(datdict.keys()), list(datdict.values())))\n",
    "    spline_fit = spline_model[i](oof_preds[i])\n",
    "    spline_fit = np.clip(spline_fit,0.025,0.975)\n",
    "    \n",
    "    print(f\"adjusted logloss of cvsplit {i}: {log_loss(np.where(y>0,1,0),spline_fit)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e0bb29-f535-4f27-9ad4-19f81c15e550",
   "metadata": {},
   "outputs": [],
   "source": [
    "spline_model = []\n",
    "\n",
    "for i in range(repeat_cv):\n",
    "    dat = list(zip(oof_preds[i],np.where(y>0,1,0)))\n",
    "    dat = sorted(dat, key = lambda x: x[0])\n",
    "    datdict = {}\n",
    "    for k in range(len(dat)):\n",
    "        datdict[dat[k][0]]= dat[k][1]\n",
    "    spline_model.append(UnivariateSpline(list(datdict.keys()), list(datdict.values())))\n",
    "    spline_fit = spline_model[i](oof_preds[i])\n",
    "    spline_fit = np.clip(spline_fit,0.025,0.975)\n",
    "    spline_fit[(tourney_data.T1_seed==1) & (tourney_data.T2_seed==16)] = 1.0\n",
    "    spline_fit[(tourney_data.T1_seed==2) & (tourney_data.T2_seed==15)] = 1.0\n",
    "    spline_fit[(tourney_data.T1_seed==3) & (tourney_data.T2_seed==14)] = 1.0\n",
    "    spline_fit[(tourney_data.T1_seed==4) & (tourney_data.T2_seed==13)] = 1.0\n",
    "    spline_fit[(tourney_data.T1_seed==16) & (tourney_data.T2_seed==1)] = 0.0\n",
    "    spline_fit[(tourney_data.T1_seed==15) & (tourney_data.T2_seed==2)] = 0.0\n",
    "    spline_fit[(tourney_data.T1_seed==14) & (tourney_data.T2_seed==3)] = 0.0\n",
    "    spline_fit[(tourney_data.T1_seed==13) & (tourney_data.T2_seed==4)] = 0.0\n",
    "    \n",
    "    print(f\"adjusted logloss of cvsplit {i}: {log_loss(np.where(y>0,1,0),spline_fit)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f86d78-fb65-4ccf-a5f1-157b2e0823b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#looking for upsets\n",
    "pd.concat(\n",
    "    [tourney_data[(tourney_data.T1_seed==1) & (tourney_data.T2_seed==16) & (tourney_data.T1_Score < tourney_data.T2_Score)],\n",
    "     tourney_data[(tourney_data.T1_seed==2) & (tourney_data.T2_seed==15) & (tourney_data.T1_Score < tourney_data.T2_Score)],\n",
    "     tourney_data[(tourney_data.T1_seed==3) & (tourney_data.T2_seed==14) & (tourney_data.T1_Score < tourney_data.T2_Score)],\n",
    "     tourney_data[(tourney_data.T1_seed==4) & (tourney_data.T2_seed==13) & (tourney_data.T1_Score < tourney_data.T2_Score)],\n",
    "     tourney_data[(tourney_data.T1_seed==16) & (tourney_data.T2_seed==1) & (tourney_data.T1_Score > tourney_data.T2_Score)],\n",
    "     tourney_data[(tourney_data.T1_seed==15) & (tourney_data.T2_seed==2) & (tourney_data.T1_Score > tourney_data.T2_Score)],\n",
    "     tourney_data[(tourney_data.T1_seed==14) & (tourney_data.T2_seed==3) & (tourney_data.T1_Score > tourney_data.T2_Score)],\n",
    "     tourney_data[(tourney_data.T1_seed==13) & (tourney_data.T2_seed==4) & (tourney_data.T1_Score > tourney_data.T2_Score)]]\n",
    ")   \n",
    "\n",
    "#https://en.wikipedia.org/wiki/NCAA_Division_I_Women%27s_Basketball_Tournament_upsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f0a369-df4f-4f17-b3b7-b4e0a6e1fd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "spline_model = []\n",
    "\n",
    "for i in range(repeat_cv):\n",
    "    dat = list(zip(oof_preds[i],np.where(y>0,1,0)))\n",
    "    dat = sorted(dat, key = lambda x: x[0])\n",
    "    datdict = {}\n",
    "    for k in range(len(dat)):\n",
    "        datdict[dat[k][0]]= dat[k][1]\n",
    "    spline_model.append(UnivariateSpline(list(datdict.keys()), list(datdict.values())))\n",
    "    spline_fit = spline_model[i](oof_preds[i])\n",
    "    spline_fit = np.clip(spline_fit,0.025,0.975)\n",
    "    spline_fit[(tourney_data.T1_seed==1) & (tourney_data.T2_seed==16) & (tourney_data.T1_Score > tourney_data.T2_Score)] = 1.0\n",
    "    spline_fit[(tourney_data.T1_seed==2) & (tourney_data.T2_seed==15) & (tourney_data.T1_Score > tourney_data.T2_Score)] = 1.0\n",
    "    spline_fit[(tourney_data.T1_seed==3) & (tourney_data.T2_seed==14) & (tourney_data.T1_Score > tourney_data.T2_Score)] = 1.0\n",
    "    spline_fit[(tourney_data.T1_seed==4) & (tourney_data.T2_seed==13) & (tourney_data.T1_Score > tourney_data.T2_Score)] = 1.0\n",
    "    spline_fit[(tourney_data.T1_seed==16) & (tourney_data.T2_seed==1) & (tourney_data.T1_Score < tourney_data.T2_Score)] = 0.0\n",
    "    spline_fit[(tourney_data.T1_seed==15) & (tourney_data.T2_seed==2) & (tourney_data.T1_Score < tourney_data.T2_Score)] = 0.0\n",
    "    spline_fit[(tourney_data.T1_seed==14) & (tourney_data.T2_seed==3) & (tourney_data.T1_Score < tourney_data.T2_Score)] = 0.0\n",
    "    spline_fit[(tourney_data.T1_seed==13) & (tourney_data.T2_seed==4) & (tourney_data.T1_Score < tourney_data.T2_Score)] = 0.0\n",
    "    \n",
    "    print(f\"adjusted logloss of cvsplit {i}: {log_loss(np.where(y>0,1,0),spline_fit)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127a0652-4fe8-4420-8ed4-e35fcf212572",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_cv = []\n",
    "spline_model = []\n",
    "\n",
    "for i in range(repeat_cv):\n",
    "    dat = list(zip(oof_preds[i],np.where(y>0,1,0)))\n",
    "    dat = sorted(dat, key = lambda x: x[0])\n",
    "    datdict = {}\n",
    "    for k in range(len(dat)):\n",
    "        datdict[dat[k][0]]= dat[k][1]\n",
    "    spline_model.append(UnivariateSpline(list(datdict.keys()), list(datdict.values())))\n",
    "    spline_fit = spline_model[i](oof_preds[i])\n",
    "    spline_fit = np.clip(spline_fit,0.025,0.975)\n",
    "    spline_fit[(tourney_data.T1_seed==1) & (tourney_data.T2_seed==16) & (tourney_data.T1_Score > tourney_data.T2_Score)] = 1.0\n",
    "    spline_fit[(tourney_data.T1_seed==2) & (tourney_data.T2_seed==15) & (tourney_data.T1_Score > tourney_data.T2_Score)] = 1.0\n",
    "    spline_fit[(tourney_data.T1_seed==3) & (tourney_data.T2_seed==14) & (tourney_data.T1_Score > tourney_data.T2_Score)] = 1.0\n",
    "    spline_fit[(tourney_data.T1_seed==4) & (tourney_data.T2_seed==13) & (tourney_data.T1_Score > tourney_data.T2_Score)] = 1.0\n",
    "    spline_fit[(tourney_data.T1_seed==16) & (tourney_data.T2_seed==1) & (tourney_data.T1_Score < tourney_data.T2_Score)] = 0.0\n",
    "    spline_fit[(tourney_data.T1_seed==15) & (tourney_data.T2_seed==2) & (tourney_data.T1_Score < tourney_data.T2_Score)] = 0.0\n",
    "    spline_fit[(tourney_data.T1_seed==14) & (tourney_data.T2_seed==3) & (tourney_data.T1_Score < tourney_data.T2_Score)] = 0.0\n",
    "    spline_fit[(tourney_data.T1_seed==13) & (tourney_data.T2_seed==4) & (tourney_data.T1_Score < tourney_data.T2_Score)] = 0.0\n",
    "    \n",
    "    val_cv.append(pd.DataFrame({\"y\":np.where(y>0,1,0), \"pred\":spline_fit, \"season\":tourney_data.Season}))\n",
    "    print(f\"adjusted logloss of cvsplit {i}: {log_loss(np.where(y>0,1,0),spline_fit)}\") \n",
    "    \n",
    "val_cv = pd.concat(val_cv)\n",
    "val_cv.groupby('season').apply(lambda x: log_loss(x.y, x.pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9feb6b-e89b-48f1-89ca-6f7bfa5d1214",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **Submission time**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c161cdc8-83ad-4150-8199-247e10e33ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "sub[\"Season\"] = 2018\n",
    "sub[\"T1_TeamID\"] = sub[\"ID\"].apply(lambda x: x[5:9]).astype(int)\n",
    "sub[\"T2_TeamID\"] = sub[\"ID\"].apply(lambda x: x[10:14]).astype(int)\n",
    "sub.head()\n",
    "\"\"\"\n",
    "sub = pd.read_csv(DATA_PATH + \"SampleSubmission2023.csv\")\n",
    "sub['Season'] = sub['ID'].apply(lambda x: int(x.split('_')[0]))\n",
    "sub[\"T1_TeamID\"] = sub['ID'].apply(lambda x: int(x.split('_')[1]))\n",
    "sub[\"T2_TeamID\"] = sub['ID'].apply(lambda x: int(x.split('_')[2]))\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d75088e-f56c-4688-b72b-68792f7d7f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds_T1.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40ad9f6-090a-4b02-a974-09498797342c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.merge(sub, season_statistics_T1, on = ['Season', 'T1_TeamID'], how = 'left')\n",
    "sub = pd.merge(sub, season_statistics_T2, on = ['Season', 'T2_TeamID'], how = 'left')\n",
    "\n",
    "sub = pd.merge(sub, glm_quality_T1, on = ['Season', 'T1_TeamID'], how = 'left')\n",
    "\n",
    "sub = pd.merge(sub, glm_quality_T2, on = ['Season', 'T2_TeamID'], how = 'left')\n",
    "\n",
    "sub = pd.merge(sub, seeds_T1, on = ['Season', 'T1_TeamID'], how = 'left')\n",
    "sub = pd.merge(sub, seeds_T2, on = ['Season', 'T2_TeamID'], how = 'left')\n",
    "sub = pd.merge(sub, last14days_stats_T1, on = ['Season', 'T1_TeamID'], how = 'left')\n",
    "sub = pd.merge(sub, last14days_stats_T2, on = ['Season', 'T2_TeamID'], how = 'left')\n",
    "\n",
    "sub[\"Seed_diff\"] = sub[\"T1_seed\"] - sub[\"T2_seed\"]\n",
    "\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfb484d-135e-4972-a867-de37a50c3149",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xsub = sub[features].values\n",
    "dtest = xgb.DMatrix(Xsub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3a0baf-f8f5-4876-b091-043800a45130",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_models = []\n",
    "for i in range(repeat_cv):\n",
    "    print(f\"Fold repeater {i}\")\n",
    "    sub_models.append(\n",
    "        xgb.train(\n",
    "          params = param,\n",
    "          dtrain = dtrain,\n",
    "          num_boost_round = int(iteration_counts[i] * 1.05),\n",
    "          verbose_eval = 50\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957912f0-7a9d-4bce-b259-fca4bbcfb3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_preds = []\n",
    "for i in range(repeat_cv):\n",
    "    sub_preds.append(np.clip(spline_model[i](np.clip(sub_models[i].predict(dtest),-30,30)),0.025,0.975))\n",
    "    \n",
    "sub[\"Pred\"] = pd.DataFrame(sub_preds).mean(axis=0)\n",
    "\"\"\"\n",
    "sub.loc[(sub.T1_seed==1) & (sub.T2_seed==16), 'Pred'] = 1.0\n",
    "sub.loc[(sub.T1_seed==2) & (sub.T2_seed==15), 'Pred'] = 1.0\n",
    "sub.loc[(sub.T1_seed==3) & (sub.T2_seed==14), 'Pred'] = 1.0\n",
    "sub.loc[(sub.T1_seed==4) & (sub.T2_seed==13), 'Pred'] = 1.0\n",
    "sub.loc[(sub.T1_seed==16) & (sub.T2_seed==1), 'Pred'] = 0.0\n",
    "sub.loc[(sub.T1_seed==15) & (sub.T2_seed==2), 'Pred'] = 0.0\n",
    "sub.loc[(sub.T1_seed==14) & (sub.T2_seed==3), 'Pred'] = 0.0\n",
    "sub.loc[(sub.T1_seed==13) & (sub.T2_seed==4), 'Pred'] = 0.0\n",
    "\"\"\"\n",
    "sub[['ID','Pred']].to_csv(\"submission.csv\", index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2757a84-8a0e-4ec3-a0c0-86b9f5446350",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1549fd-919f-4ebd-b869-efa6291da099",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "tourney_results2018 = pd.read_csv('../input/NCAA_2018_Solution_Womens.csv')\n",
    "tourney_results2018 = tourney_results2018[tourney_results2018.Pred!=-1].reset_index(drop=True)\n",
    "tourney_results2018.columns = ['ID', 'label']\n",
    "tourney_results2018 = pd.merge(tourney_results2018, sub, on = 'ID')\n",
    "log_loss(tourney_results2018.label, tourney_results2018.Pred)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcb4119-93ac-47e5-b9f9-6b495a9a1bc4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **Notes from submitter (post-competition)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7642d708-eafc-45d6-b10d-a1336c2451f8",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **1st Place Submission - Another victory for raddars code**\n",
    "\n",
    "** I may be a bit early with this post but will update if anything ends up changing.\n",
    "\n",
    "As I had noted in a prior post my submission was essentially the @raddar code from a few years back (the python version instead of the R version that is widely used). Initially I was planning on using the base code and updating the features however I ended up not having the time.\n",
    "\n",
    "I haven’t had much time for competitions over the last year or two however the March Madness one is always a lot of fun to come back to. Ultimately I was more lucky than anything else as I didn’t add much past updating the code to work with the current year’s data. Basically I just verified the resulting predictions seemed appropriate and went with the results.\n",
    "\n",
    "For fun I used the same predictions in the ESPN Bracket Challenge and ended up in the 50% percentile in the Men’s and the 97% percentile in the Women’s.\n",
    "\n",
    "The notebook for my submission can be found here (Paris Madness 2023).\n",
    "\n",
    "In keeping with my submission maybe I should have gone with less effort and had ChatGPT provide some commentary. Note: I actually did this asking for some witty commentary that ended up being far more interesting (make sure to check out the bottom of this post)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18caeab-2edc-414d-a031-e38180e12b2c",
   "metadata": {},
   "source": [
    "#### **Submissions**\n",
    "\n",
    "Best Submission: My contribution was limited to 1) commenting out the np.exp() line when calculating Team Quality as it ended up returning quite a few inf values and 2) No overrides in match-ups of seeds 1-4 against seeds 13-16.\n",
    "\n",
    "Without the changes above the model still performed really well and ended up with a score of 0.17629 which would have been good enough for 6th place on the public leaderboard at the end of the competition.\n",
    "\n",
    "Although the code was pretty similar to the R version that has resulted in top finishes over the last few years I thought I’d provide a bit of commentary on the Python version. Guess I feel a bit guilty about the amount of effort I put into my submission that somehow ended up winning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc68f8fb-e6e0-43a9-bb85-d7771eee5577",
   "metadata": {},
   "source": [
    "#### **Features**\n",
    "\n",
    "Mean of the following regular season stats for each team (where T2 is the opponent of T1). So basically these are repeated four times (team 1, team 1 opponents, team 2 and team 2 opponents). Of the features below only 'PointDiff' was a calculated variable.\n",
    "\n",
    "`['T1_FGM', 'T1_FGA', 'T1_FGM3', 'T1_FGA3', 'T1_OR', 'T1_Ast','T1_TO', 'T1_Stl', 'T1_PF', 'T2_FGM', 'T2_FGA', 'T2_FGM3', 'T2_FGA3', 'T2_OR', 'T2_Ast', 'T2_TO', 'T2_Stl','T2_Blk','PointDiff']`\n",
    "\n",
    "The only other features used in the model were the following with the win ratio over the 14 days being common in many of the models made public. \n",
    "`[‘T1_win_ratio_14d’, ‘T2_win_ratio_14d’, ‘T1_quality’, ‘T2_quality’, ‘Seed_diff’, ‘T1_seed’, ‘T2_seed’]`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bad6fbe-8c74-4bb7-9693-35b731b6cc5f",
   "metadata": {},
   "source": [
    "#### **Team Quality**\n",
    "\n",
    "One of the main items in the R version of the code was the random effects GLMM. It looks like the Python version doesn’t use random effects which would be a difference between the two. I can’t say I’m that familiar with R or the from_formula based GLM so I’m not sure how important this difference was. Also even with removing the np.exp() there were quite a few NaN values (about 25% of the training data)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552bf899-2e98-458b-9242-0a22c5276074",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **Model**\n",
    "\n",
    "XGB was used with xgb.cv(folds = KFold(n_splits = 5, shuffle = True)) that was repeated 3 times. This is different from the original R code which used a recommended repeat_cv of 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d543197-086a-43fa-b35c-61b1744ff85b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **ChatGPT WItty Commentary**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac71b1b-f9ab-4af1-9865-e4d5eead973c",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Title: A Slam Dunk of a Model: XGBoost's Swish Predictions for March Madness 2023**\n",
    "\n",
    "**Introduction:**\n",
    "\n",
    "March Madness 2023 proved to be a wild ride full of upsets, nail-biting finishes, and buzzer-beaters. But while the on-court action left fans on the edge of their seats, our trusty XGBoost model coolly calculated the winners behind the scenes. With only a few minor tweaks, our predictive powerhouse once again clinched the top spot in the Kaggle competition, leaving the rivals in the dust like an ankle-breaking crossover.\n",
    "\n",
    "**Model Overview:**\n",
    "\n",
    "XGBoost, the LeBron James of machine learning algorithms, is an ensemble learning technique famous for its slam-dunk performance across diverse tasks. Our winning model harnessed the power of XGBoost to tackle the NCAA tournament, predicting outcomes with such accuracy that even the most seasoned sports analysts would be left speechless.\n",
    "\n",
    "**Data and Features:**\n",
    "\n",
    "The model feasted on a buffet of historical NCAA data, devouring team stats, player performances, and even the odd tidbits of info like a hungry freshman at an all-you-can-eat dining hall. Our feature set comprised the crème de la crème of data points, cherry-picked for their impact on game outcomes:\n",
    "\n",
    "1) Team statistics: win-loss ratios, offensive and defensive wizardry, and average margin of victory.\n",
    "2) Player performance: on-court heroics such as points per game, rebounds, assists, and shooting percentages.\n",
    "3) Miscellaneous factors: the elusive home-court advantage, battle-hardened experience, coaching prowess, and the all-important team momentum leading up to the tournament.\n",
    "\n",
    "**Model Training and Validation:**\n",
    "\n",
    "Like a well-coached team running drills, our model underwent rigorous training and validation procedures. The dataset was divided into training and validation sets, and a cross-validation strategy was adopted to fine-tune the model's hyperparameters. By repeatedly putting our model through its paces, we pinpointed the winning combo of parameters that delivered the most accurate predictions.\n",
    "\n",
    "**Key Changes and Improvements:**\n",
    "\n",
    "Though our XGBoost model has been a perennial contender, we couldn't resist tweaking it to keep it ahead of the pack. The changes we made for this year's competition were akin to swapping out our playbook's dog-eared pages for fresh new strategies:\n",
    "\n",
    "1) Updated data: We retrained our model on the freshest NCAA tournament data, ensuring it was up to speed with the latest college basketball shenanigans.\n",
    "2) Hyperparameter tuning: A fine-tuning of our model's hyperparameters, based on the most recent competition data, turned it into an unstoppable force.\n",
    "\n",
    "**Conclusion:**\n",
    "\n",
    "Our XGBoost model's triumphant performance in the 2023 Kaggle March Madness competition is a testament to its predictive prowess. With just a few strategic adjustments and an eye for the latest trends, our model swished its way to the top, earning the title of MVP in the world of college basketball predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9186efcf-aa10-41d4-9dd9-f3fc6bbedd30",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **Selected Comment Threads**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899c09b2-d42d-42a1-9855-d9c11a8104b8",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **Thread 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f078776-154f-43a4-994d-4ae6673ea354",
   "metadata": {},
   "source": [
    "**Frankie Niemann** 11th in this Competition\n",
    "Congrats! I also used @raddar code from a few years back and removed np.exp() line for Team Quality because of the NaN and Inf values it produced. I stuck with the recommended parameters for the model. In terms of overrides, I kept the seeds 1-4 vs 13-16 overrides for the women's matchups and only made overrides for seeds 1-2 vs 15-16 for the men's matchups. I'm curious as to how much the overrides affected my final outcome, given I was so close to cracking the top 8.\n",
    "\n",
    "Reply\n",
    "**Zach Bradshaw** 183rd in this Competition\n",
    "It looks like a game that hurt your score was giving UCLA a 50% chance to beat Sacramento State. Using the same prediction as RustyB for that game would have had you in 7th. If we also undo the Princeton and F Dickinson overrides to 0% and use the same predictions as RustyB that would have you in 2nd. Under logloss scoring those two overrides would have sunk you near the bottom with such high penalties.\n",
    "\n",
    "Reply\n",
    "**Frankie Niemann** 11th in this Competition\n",
    "Thanks for taking a look at this, Zach! It appears for whatever reason my predictions omitted Sacramento State and by default all of their matchups were given a 50%. I spotted the same issue with UC Santa Barbara on the Men's side, so I made an override to their first round game to give Baylor a 100% chance of winning. Unfortunately, I didn't spot the same error for Sac State and it cost me. I guess I'll have to check my work better next year!\n",
    "\n",
    "Reply\n",
    "**RustyB** TOPIC AUTHOR 1st in this Competition\n",
    "This is so interesting and unfortunate/unlucky. If I had more time I'm assuming I would have made changes that dropped me way down the leaderboard. Looking forward to trying them now that late submissions are open."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5a2c23-7172-483a-964d-b547c6225c60",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **Thread 2**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f36d09d-6d6e-432d-813c-380bbd539e92",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "**raddar** 129th in this Competition\n",
    "I want to reflect a bit about the 2018 python code. It was the very first time I used python in competitions (was 100% R user then); So it is very special to me. And to be honest, I would no longer be able to replicate the R version of it (never touched R since).\n",
    "\n",
    "If I am not mistaken, the python code has a small pesky bug in it, when it comes to doing pandas joins. Not sure if I have fixed it :)\n",
    "\n",
    "Reply\n",
    "**RustyB** TOPIC AUTHOR 1st in this Competition\n",
    "I wouldn't be surprised if there was an unintended merge or join in there someplace. I don't know R myself so I'm not easily able to follow through the columns of data through each step. Might just have to chalk it up to one of life's great mysteries.\n",
    "\n",
    "I'm hoping to python refactor the code to make it a bit easier to use prior to next year (there is likely a version out there already but I'll make mine public as well) I'm likely not going to get that in depth into the R version as I simply don't use it at all.\n",
    "\n",
    "Reply\n",
    "**RustyB** TOPIC AUTHOR 1st in this Competition\n",
    "For anyone interested I may have stumbled on the merging error. I realized the notebook only calculates Team Quality from 2010 and after. When adding in the prior periods 380 rows get added to training data (all of these are in 2009 for a total of 508 data points for 2009).\n",
    "\n",
    "Haven't had a chance to look at the detail at all but it gets added with the following merge.\n",
    "`tourney_data = pd.merge(tourney_data, seeds_T1, on = ['Season', 'T1_TeamID'], how = 'left')\n",
    "tourney_data = pd.merge(tourney_data, seeds_T2, on = ['Season', 'T2_TeamID'], how = 'left')`\n",
    "\n",
    "As the data is added to 2009 I'm assuming this is why the Team Quality calculations started in 2010. For this year's tournament it would have actually worked out in a better result (got a score of 0.1688 with Team Quality used in all periods).\n",
    "\n",
    "Reply\n",
    "**raddar** 129th in this Competition\n",
    "This is not it. For women's NCAA `WNCAATourneyDetailedResults.csv` is available from 2010 only - that is the main reason. Of course, now as we have men's and women's combined, it makes sense to add prior results.\n",
    "\n",
    "I double checked you work; I remember that in my first notebook versions I did not use `how='left'` which meant some inner joins would drop rows. But it seems this has been fixed in my later versions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py 3.11 (base)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
