{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "880c7434-be28-409d-a243-d1dd1d7e1591",
   "metadata": {},
   "source": [
    "### **Model Building Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "748c183b-3c52-4705-9b87-e46f28e90931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select folder path based on user input\n",
    "gender = 'M' #input('Enter gender (W for women, M for men): ')\n",
    "\n",
    "# Assign the appropriate folder path based on the input\n",
    "MAIN_DIR = './'\n",
    "USE_DIR = MAIN_DIR + 'womens/' if gender.upper() == 'W' else MAIN_DIR + 'mens/'\n",
    "PRE = 'W' if gender.upper() == 'W' else 'M'\n",
    "NAME = 'womens' if gender.upper() == 'W' else 'mens'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70ca81a4-82cb-49e8-ab02-9616bdc1f5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1ba3957-b9b0-4d82-ba68-99b40a9119d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your dataframes into variables from previous steps\n",
    "games = pd.read_csv('games-' + NAME + '.csv')\n",
    "tourney = pd.read_csv('tourney-' + NAME + '.csv')\n",
    "\n",
    "# Convert DayDate column to datetime format\n",
    "games['DayDate'] = pd.to_datetime(games['DayDate'])\n",
    "tourney['DayDate'] = pd.to_datetime(tourney['DayDate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a255b39c-657e-4588-aa95-534aada8b8c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((107634, 21), (1248, 21))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that data loaded properly\n",
    "games.shape, tourney.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d48fc54-06d6-426b-b410-4fa1ea78a613",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Season', 'Team0', 'Team1', 'DayDate', 'Seed', 'Site', 'MOV', 'FG2M',\n",
       "       'FG2A', 'FG3M', 'FG3A', 'FT1M', 'FT1A', 'ORB', 'DRB', 'AST', 'TOVR',\n",
       "       'STL', 'BLK', 'PFL', 'Outcome'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tourney.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506ffcf9-e54c-4838-9065-33475e939961",
   "metadata": {},
   "source": [
    "### **Split Data Into Train, Validation, and Test Sets**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f81e094-3bc4-4ee4-8517-692ea7c90a08",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### **Check to see how scikit learn's `feature_selection.VarianceThreshold` method selects features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4edfe768-e2d8-47e8-9076-7973630503c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(107634, 18)\n",
      "(1248, 18)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# Specify 'Games' X,y independent and dependent variables (X,y = G,g; e.g X_train = G_train)\n",
    "G = games.drop(columns=['DayDate', 'Outcome'])\n",
    "g = games['Outcome']\n",
    "\n",
    "# Specify Tourney' X,y variables (X,y = T,t; e.g X_train = T_train)\n",
    "T = tourney.drop(columns=['DayDate', 'Outcome'])\n",
    "t = tourney['Outcome']\n",
    "\n",
    "# Remove low variance features\n",
    "selection = VarianceThreshold(threshold=(0.1))    \n",
    "\n",
    "# Check 'Games' feature variance\n",
    "G = selection.fit_transform(G)\n",
    "print(G.shape) # removes 'Seed' plus two dropped columns ['DayDate', 'Outcome']\n",
    "\n",
    "# Check 'Tourney' feature variance\n",
    "T = selection.fit_transform(T)\n",
    "print(T.shape) # removes 'Site' plus two dropped columns ['DayDate', 'Outcome']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46eb165-9494-44e8-8fd6-6e26d552ce3a",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **Custom function to split data based on provided year and number of years to go back**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea8bd406-952f-473d-bc56-4de4a94da3f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Specify the season you want to use for training and testing\n",
    "\n",
    "def split_data(year, lookback):\n",
    "\n",
    "    global data\n",
    "    offset = year - 1\n",
    "\n",
    "    # Define the training dataset\n",
    "\n",
    "    # Set seasons for training dataset\n",
    "    train_seasons = []\n",
    "    for season in range(year - lookback, year + 1):\n",
    "        train_seasons.append(season)\n",
    "\n",
    "    # Create dataframe from selected seasons\n",
    "    train_df = games[games['Season'].isin(train_seasons)]\n",
    "\n",
    "    # Split dataframe into X,y\n",
    "    X_train = train_df.drop(columns=['DayDate', 'Outcome'], axis=1)\n",
    "    y_train = train_df['Outcome']\n",
    "    \n",
    "    # Define the validation dataset\n",
    "\n",
    "    # Set seasons for validation dataset\n",
    "    val_seasons = []\n",
    "    for season in range(year - lookback, year + 1):\n",
    "        val_seasons.append(season)\n",
    "\n",
    "    # Create dataframe from selected seasons\n",
    "    val_df = tourney[tourney['Season'].isin(val_seasons)]\n",
    "\n",
    "    # Split dataframe into X,y\n",
    "    X_val = val_df.drop(columns=['DayDate', 'Outcome'], axis=1)\n",
    "    y_val = val_df['Outcome']\n",
    "        \n",
    "    # Define the testing dataset\n",
    "\n",
    "    # Set seasons for testing dataset\n",
    "    test_seasons = []\n",
    "    for season in range(year, year + 1):\n",
    "        test_seasons.append(season)\n",
    "\n",
    "    # Create dataframe from selected seasons\n",
    "    test_df = games[games['Season'].isin(test_seasons)]\n",
    "\n",
    "    # Split dataframe into X,y\n",
    "    X_test = test_df.drop(columns=['DayDate', 'Outcome'], axis=1)\n",
    "    y_test = test_df['Outcome']\n",
    "    \n",
    "    data = {'X_train':X_train, 'X_val':X_val, 'X_test':X_test, 'y_train':y_train, 'y_val':y_val, 'y_test':y_test}\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "037d59b7-f272-447f-836e-95883e4c6e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_stack_data(year, lookback):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    global data_stack\n",
    "\n",
    "    # Combine season and tourney data for same period\n",
    "\n",
    "    # Set seasons for dataset\n",
    "    stack_regular = []\n",
    "    stack_tourney = []\n",
    "    for season in range(year - lookback, year + 1):\n",
    "        stack_regular.append(season)\n",
    "    for season in range(year - lookback, year):\n",
    "        stack_tourney.append(season)\n",
    "\n",
    "    # Create dataframe from selected seasons\n",
    "    stack_g = games[games['Season'].isin(stack_regular)].copy()\n",
    "    stack_t = tourney[tourney['Season'].isin(stack_tourney)].copy()\n",
    "    \n",
    "    # Concatenate data frames vertically\n",
    "    combined = pd.concat([stack_g, stack_t], ignore_index=True)\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(combined.drop(['DayDate', 'Outcome'], axis=1), \n",
    "                                                        combined['Outcome'], test_size=0.2, random_state=42)\n",
    "\n",
    "    data_stack = {'X_train':X_train, 'X_test':X_test, 'y_train':y_train, 'y_test':y_test}\n",
    "    \n",
    "    return data_stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f47e1e9-5c5a-4cde-9bef-40614f31a9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_data(2023, 3); # Custom function to split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "326343e5-42af-4e62-aaa3-bfc6aabe3538",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((20130, 19), (133, 19), (5602, 19))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rename Train, Val, Test data\n",
    "X_train, X_val, X_test = data['X_train'], data['X_val'], data['X_test']\n",
    "y_train, y_val, y_test = data['y_train'], data['y_val'], data['y_test']\n",
    "\n",
    "X_train.shape, X_val.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b414156-51fc-48be-8249-eb758426fe38",
   "metadata": {},
   "source": [
    "### **Build Classification Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d316d94a-6621-46e0-ae9b-fdae48ed6522",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **Select metrics; Custom functions for Brier Score metric**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e694f89-5e0a-42fb-9139-9e71e6463739",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import brier_score_loss, accuracy_score, matthews_corrcoef, f1_score\n",
    "from keras.metrics import binary_accuracy\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e584b38d-1c4e-41cf-8535-de9ad62c69e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Brier score function\n",
    "def brier_score(y_true, y_pred):\n",
    "    return K.mean(K.square(y_true - y_pred), axis=-1)\n",
    "\n",
    "# Define custom metric function wrapper\n",
    "def brier_metric(y_true, y_pred):\n",
    "    return 1 - brier_score(y_true, y_pred)\n",
    "\n",
    "# Define Brier score function\n",
    "def brier_score_metric(y_true, y_pred):\n",
    "    return K.mean(K.square(y_true - y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6db98d1-62a8-468a-8290-bce9a28fa0f3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **Support Vector Machine Model (radial basis function kernel)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ac46d56-6abb-4828-ae5d-322008d9bcd7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model performance for Training set\n",
      "- Brier: 2.7486432965980435e-05\n",
      "- Accuracy: 1.0\n",
      "- MCC: 1.0\n",
      "- F1 score: 1.0\n",
      "----------------------------------\n",
      "Model performance for Validation set\n",
      "- Brier: 2.753414520708321e-05\n",
      "- Accuracy: 1.0\n",
      "- MCC: 1.0\n",
      "- F1 score: 1.0\n",
      "----------------------------------\n",
      "Model performance for Test set\n",
      "- Brier: 2.7486432965980435e-05\n",
      "- Accuracy: 1.0\n",
      "- MCC: 1.0\n",
      "- F1 score: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm = SVC(kernel='linear', probability=True)\n",
    "\n",
    "# Train model\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "svm_train_pred = svm.predict(X_train)\n",
    "svm_val_pred = svm.predict(X_val)\n",
    "svm_test_pred = svm.predict(X_test)\n",
    "\n",
    "# Make probabilities\n",
    "svm_train_prob = svm.predict_proba(X_train)[:, 1]\n",
    "svm_val_prob = svm.predict_proba(X_val)[:, 1]\n",
    "svm_test_prob = svm.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Training set performance\n",
    "svm_train_brier = brier_score_loss(y_train, svm_train_prob)\n",
    "svm_train_accuracy = accuracy_score(y_train, svm_train_pred)\n",
    "svm_train_mcc = matthews_corrcoef(y_train, svm_train_pred)\n",
    "svm_train_f1 = f1_score(y_train, svm_train_pred, average='weighted')\n",
    "\n",
    "# Validation set performance\n",
    "svm_val_brier = brier_score_loss(y_val, svm_val_prob)\n",
    "svm_val_accuracy = accuracy_score(y_val, svm_val_pred)\n",
    "svm_val_mcc = matthews_corrcoef(y_val, svm_val_pred)\n",
    "svm_val_f1 = f1_score(y_val, svm_val_pred, average='weighted')\n",
    "\n",
    "# Test set performance\n",
    "svm_test_brier = brier_score_loss(y_test, svm_test_prob)\n",
    "svm_test_accuracy = accuracy_score(y_test, svm_test_pred)\n",
    "svm_test_mcc = matthews_corrcoef(y_test, svm_test_pred)\n",
    "svm_test_f1 = f1_score(y_test, svm_test_pred, average='weighted')\n",
    "\n",
    "print('Model performance for Training set')\n",
    "print('- Brier: %s' % svm_train_brier)\n",
    "print('- Accuracy: %s' % svm_train_accuracy)\n",
    "print('- MCC: %s' % svm_train_mcc)\n",
    "print('- F1 score: %s' % svm_train_f1)\n",
    "print('----------------------------------')\n",
    "print('Model performance for Validation set')\n",
    "print('- Brier: %s' % svm_val_brier)\n",
    "print('- Accuracy: %s' % svm_val_accuracy)\n",
    "print('- MCC: %s' % svm_val_mcc)\n",
    "print('- F1 score: %s' % svm_val_f1)\n",
    "print('----------------------------------')\n",
    "print('Model performance for Test set')\n",
    "print('- Brier: %s' % svm_train_brier)\n",
    "print('- Accuracy: %s' % svm_test_accuracy)\n",
    "print('- MCC: %s' % svm_test_mcc)\n",
    "print('- F1 score: %s' % svm_test_f1)\n",
    "\n",
    "# save the model to a file\n",
    "svm_model = './models/svm-model.sav'\n",
    "pickle.dump(svm, open(svm_model, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbfee0e-6794-487c-97f7-0efb632f923f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **K Nearest Neighbors Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4743dfe0-5e80-47ee-b84a-856f6eaffcfb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model performance for Training set\n",
      "- Brier: 0.028234475906607055\n",
      "- Accuracy: 0.9690511674118232\n",
      "- MCC: 0.9380381167989247\n",
      "- F1 score: 0.9690493556340358\n",
      "----------------------------------\n",
      "Model performance for Validation set\n",
      "- Brier: 0.05172932330827068\n",
      "- Accuracy: 0.9473684210526315\n",
      "- MCC: 0.8946297104615851\n",
      "- F1 score: 0.9473505323383233\n",
      "----------------------------------\n",
      "Model performance for Test set\n",
      "- Brier: 0.028234475906607055\n",
      "- Accuracy: 0.9675116029989289\n",
      "- MCC: 0.9349235013292083\n",
      "- F1 score: 0.9675058978031689\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "knn = KNeighborsClassifier(5) # Define classifier\n",
    "\n",
    "# Train model\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "knn_train_pred = knn.predict(X_train)\n",
    "knn_val_pred = knn.predict(X_val)\n",
    "knn_test_pred = knn.predict(X_test)\n",
    "\n",
    "# Make probabilities\n",
    "knn_train_prob = knn.predict_proba(X_train)[:, 1]\n",
    "knn_val_prob = knn.predict_proba(X_val)[:, 1]\n",
    "knn_test_prob = knn.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Training set performance\n",
    "knn_train_brier = brier_score_loss(y_train, knn_train_prob)\n",
    "knn_train_accuracy = accuracy_score(y_train, knn_train_pred)\n",
    "knn_train_mcc = matthews_corrcoef(y_train, knn_train_pred)\n",
    "knn_train_f1 = f1_score(y_train, knn_train_pred, average='weighted')\n",
    "\n",
    "# Validation set performance\n",
    "knn_val_brier = brier_score_loss(y_val, knn_val_prob)\n",
    "knn_val_accuracy = accuracy_score(y_val, knn_val_pred)\n",
    "knn_val_mcc = matthews_corrcoef(y_val, knn_val_pred)\n",
    "knn_val_f1 = f1_score(y_val, knn_val_pred, average='weighted')\n",
    "\n",
    "# Test set performance\n",
    "knn_test_brier = brier_score_loss(y_test, knn_test_prob)\n",
    "knn_test_accuracy = accuracy_score(y_test, knn_test_pred)\n",
    "knn_test_mcc = matthews_corrcoef(y_test, knn_test_pred)\n",
    "knn_test_f1 = f1_score(y_test, knn_test_pred, average='weighted')\n",
    "\n",
    "print('Model performance for Training set')\n",
    "print('- Brier: %s' % knn_train_brier)\n",
    "print('- Accuracy: %s' % knn_train_accuracy)\n",
    "print('- MCC: %s' % knn_train_mcc)\n",
    "print('- F1 score: %s' % knn_train_f1)\n",
    "print('----------------------------------')\n",
    "print('Model performance for Validation set')\n",
    "print('- Brier: %s' % knn_val_brier)\n",
    "print('- Accuracy: %s' % knn_val_accuracy)\n",
    "print('- MCC: %s' % knn_val_mcc)\n",
    "print('- F1 score: %s' % knn_val_f1)\n",
    "print('----------------------------------')\n",
    "print('Model performance for Test set')\n",
    "print('- Brier: %s' % knn_train_brier)\n",
    "print('- Accuracy: %s' % knn_test_accuracy)\n",
    "print('- MCC: %s' % knn_test_mcc)\n",
    "print('- F1 score: %s' % knn_test_f1)\n",
    "\n",
    "# save the model to a file\n",
    "knn_model = './models/knn-model.sav'\n",
    "pickle.dump(knn, open(knn_model, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586b07cc-b8f2-4844-8b10-6d6311ea69f7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **Naive Bayes Model (Gaussian)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c23fb8cd-5dcc-4734-84c9-0bbff77a5147",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model performance for Training set\n",
      "- Brier: 0.02758298982492225\n",
      "- Accuracy: 0.962046696472926\n",
      "- MCC: 0.9240196564392616\n",
      "- F1 score: 0.9620474052496035\n",
      "----------------------------------\n",
      "Model performance for Validation set\n",
      "- Brier: 0.0296741742301681\n",
      "- Accuracy: 0.9624060150375939\n",
      "- MCC: 0.9248529693696184\n",
      "- F1 score: 0.9624145238329231\n",
      "----------------------------------\n",
      "Model performance for Test set\n",
      "- Brier: 0.02758298982492225\n",
      "- Accuracy: 0.9593002499107461\n",
      "- MCC: 0.9184746737351663\n",
      "- F1 score: 0.959301980094881\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "gnb = GaussianNB()\n",
    "\n",
    "# Train model\n",
    "gnb.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "gnb_train_pred = gnb.predict(X_train)\n",
    "gnb_val_pred = gnb.predict(X_val)\n",
    "gnb_test_pred = gnb.predict(X_test)\n",
    "\n",
    "# Make probabilities\n",
    "gnb_train_prob = gnb.predict_proba(X_train)[:, 1]\n",
    "gnb_val_prob = gnb.predict_proba(X_val)[:, 1]\n",
    "gnb_test_prob = gnb.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Training set performance\n",
    "gnb_train_brier = brier_score_loss(y_train, gnb_train_prob)\n",
    "gnb_train_accuracy = accuracy_score(y_train, gnb_train_pred)\n",
    "gnb_train_mcc = matthews_corrcoef(y_train, gnb_train_pred)\n",
    "gnb_train_f1 = f1_score(y_train, gnb_train_pred, average='weighted')\n",
    "\n",
    "# Validation set performance\n",
    "gnb_val_brier = brier_score_loss(y_val, gnb_val_prob)\n",
    "gnb_val_accuracy = accuracy_score(y_val, gnb_val_pred)\n",
    "gnb_val_mcc = matthews_corrcoef(y_val, gnb_val_pred)\n",
    "gnb_val_f1 = f1_score(y_val, gnb_val_pred, average='weighted')\n",
    "\n",
    "# Test set performance\n",
    "gnb_test_brier = brier_score_loss(y_test, gnb_test_prob)\n",
    "gnb_test_accuracy = accuracy_score(y_test, gnb_test_pred)\n",
    "gnb_test_mcc = matthews_corrcoef(y_test, gnb_test_pred)\n",
    "gnb_test_f1 = f1_score(y_test, gnb_test_pred, average='weighted')\n",
    "\n",
    "print('Model performance for Training set')\n",
    "print('- Brier: %s' % gnb_train_brier)\n",
    "print('- Accuracy: %s' % gnb_train_accuracy)\n",
    "print('- MCC: %s' % gnb_train_mcc)\n",
    "print('- F1 score: %s' % gnb_train_f1)\n",
    "print('----------------------------------')\n",
    "print('Model performance for Validation set')\n",
    "print('- Brier: %s' % gnb_val_brier)\n",
    "print('- Accuracy: %s' % gnb_val_accuracy)\n",
    "print('- MCC: %s' % gnb_val_mcc)\n",
    "print('- F1 score: %s' % gnb_val_f1)\n",
    "print('----------------------------------')\n",
    "print('Model performance for Test set')\n",
    "print('- Brier: %s' % gnb_train_brier)\n",
    "print('- Accuracy: %s' % gnb_test_accuracy)\n",
    "print('- MCC: %s' % gnb_test_mcc)\n",
    "print('- F1 score: %s' % gnb_test_f1)\n",
    "\n",
    "# save the model to a file\n",
    "gnb_model = './models/gnb-model.sav'\n",
    "pickle.dump(gnb, open(gnb_model, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d07f82-9b13-48be-b422-8bb09fb8e47d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### **Decision Tree Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "79f9ae62-8d3d-4080-b8a3-ad610389d9af",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model performance for Training set\n",
      "- Brier: 0.0\n",
      "- Accuracy: 1.0\n",
      "- MCC: 1.0\n",
      "- F1 score: 1.0\n",
      "----------------------------------\n",
      "Model performance for Validation set\n",
      "- Brier: 0.0\n",
      "- Accuracy: 1.0\n",
      "- MCC: 1.0\n",
      "- F1 score: 1.0\n",
      "----------------------------------\n",
      "Model performance for Test set\n",
      "- Brier: 0.0\n",
      "- Accuracy: 1.0\n",
      "- MCC: 1.0\n",
      "- F1 score: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt = DecisionTreeClassifier(max_depth=5) # Define classifier\n",
    "\n",
    "# Train model\n",
    "dt.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "dt_train_pred = dt.predict(X_train)\n",
    "dt_val_pred = dt.predict(X_val)\n",
    "dt_test_pred = dt.predict(X_test)\n",
    "\n",
    "# Make probabilities\n",
    "dt_train_prob = dt.predict_proba(X_train)[:, 1]\n",
    "dt_val_prob = dt.predict_proba(X_val)[:, 1]\n",
    "dt_test_prob = dt.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Training set performance\n",
    "dt_train_brier = brier_score_loss(y_train, dt_train_prob)\n",
    "dt_train_accuracy = accuracy_score(y_train, dt_train_pred)\n",
    "dt_train_mcc = matthews_corrcoef(y_train, dt_train_pred)\n",
    "dt_train_f1 = f1_score(y_train, dt_train_pred, average='weighted')\n",
    "\n",
    "# Validation set performance\n",
    "dt_val_brier = brier_score_loss(y_val, dt_val_prob)\n",
    "dt_val_accuracy = accuracy_score(y_val, dt_val_pred)\n",
    "dt_val_mcc = matthews_corrcoef(y_val, dt_val_pred)\n",
    "dt_val_f1 = f1_score(y_val, dt_val_pred, average='weighted')\n",
    "\n",
    "# Test set performance\n",
    "dt_test_brier = brier_score_loss(y_test, dt_test_prob)\n",
    "dt_test_accuracy = accuracy_score(y_test, dt_test_pred)\n",
    "dt_test_mcc = matthews_corrcoef(y_test, dt_test_pred)\n",
    "dt_test_f1 = f1_score(y_test, dt_test_pred, average='weighted')\n",
    "\n",
    "print('Model performance for Training set')\n",
    "print('- Brier: %s' % dt_train_brier)\n",
    "print('- Accuracy: %s' % dt_train_accuracy)\n",
    "print('- MCC: %s' % dt_train_mcc)\n",
    "print('- F1 score: %s' % dt_train_f1)\n",
    "print('----------------------------------')\n",
    "print('Model performance for Validation set')\n",
    "print('- Brier: %s' % dt_val_brier)\n",
    "print('- Accuracy: %s' % dt_val_accuracy)\n",
    "print('- MCC: %s' % dt_val_mcc)\n",
    "print('- F1 score: %s' % dt_val_f1)\n",
    "print('----------------------------------')\n",
    "print('Model performance for Test set')\n",
    "print('- Brier: %s' % dt_train_brier)\n",
    "print('- Accuracy: %s' % dt_test_accuracy)\n",
    "print('- MCC: %s' % dt_test_mcc)\n",
    "print('- F1 score: %s' % dt_test_f1)\n",
    "\n",
    "# save the model to a file\n",
    "dt_model = './models/dt-model.sav'\n",
    "pickle.dump(dt, open(dt_model, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f7aede-336b-4bbd-b72d-effd279c9102",
   "metadata": {},
   "source": [
    "### **Random Forest Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d3088d1-e605-45ce-98aa-1c269b2ed6ee",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model performance for Training set\n",
      "- Brier: 0.00032041728763040233\n",
      "- Accuracy: 1.0\n",
      "- MCC: 1.0\n",
      "- F1 score: 1.0\n",
      "----------------------------------\n",
      "Model performance for Validation set\n",
      "- Brier: 0.0007518796992481201\n",
      "- Accuracy: 1.0\n",
      "- MCC: 1.0\n",
      "- F1 score: 1.0\n",
      "----------------------------------\n",
      "Model performance for Test set\n",
      "- Brier: 0.00032041728763040233\n",
      "- Accuracy: 1.0\n",
      "- MCC: 1.0\n",
      "- F1 score: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=10) # Define classifier\n",
    "\n",
    "# Train model\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "rf_train_pred = rf.predict(X_train)\n",
    "rf_val_pred = rf.predict(X_val)\n",
    "rf_test_pred = rf.predict(X_test)\n",
    "\n",
    "# Make probabilities\n",
    "rf_train_prob = rf.predict_proba(X_train)[:, 1]\n",
    "rf_val_prob = rf.predict_proba(X_val)[:, 1]\n",
    "rf_test_prob = rf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Training set performance\n",
    "rf_train_brier = brier_score_loss(y_train, rf_train_prob)\n",
    "rf_train_accuracy = accuracy_score(y_train, rf_train_pred)\n",
    "rf_train_mcc = matthews_corrcoef(y_train, rf_train_pred)\n",
    "rf_train_f1 = f1_score(y_train, rf_train_pred, average='weighted')\n",
    "\n",
    "# Validation set performance\n",
    "rf_val_brier = brier_score_loss(y_val, rf_val_prob)\n",
    "rf_val_accuracy = accuracy_score(y_val, rf_val_pred)\n",
    "rf_val_mcc = matthews_corrcoef(y_val, rf_val_pred)\n",
    "rf_val_f1 = f1_score(y_val, rf_val_pred, average='weighted')\n",
    "\n",
    "# Test set performance\n",
    "rf_test_brier = brier_score_loss(y_test, rf_test_prob)\n",
    "rf_test_accuracy = accuracy_score(y_test, rf_test_pred)\n",
    "rf_test_mcc = matthews_corrcoef(y_test, rf_test_pred)\n",
    "rf_test_f1 = f1_score(y_test, rf_test_pred, average='weighted')\n",
    "\n",
    "print('Model performance for Training set')\n",
    "print('- Brier: %s' % rf_train_brier)\n",
    "print('- Accuracy: %s' % rf_train_accuracy)\n",
    "print('- MCC: %s' % rf_train_mcc)\n",
    "print('- F1 score: %s' % rf_train_f1)\n",
    "print('----------------------------------')\n",
    "print('Model performance for Validation set')\n",
    "print('- Brier: %s' % rf_val_brier)\n",
    "print('- Accuracy: %s' % rf_val_accuracy)\n",
    "print('- MCC: %s' % rf_val_mcc)\n",
    "print('- F1 score: %s' % rf_val_f1)\n",
    "print('----------------------------------')\n",
    "print('Model performance for Test set')\n",
    "print('- Brier: %s' % rf_train_brier)\n",
    "print('- Accuracy: %s' % rf_test_accuracy)\n",
    "print('- MCC: %s' % rf_test_mcc)\n",
    "print('- F1 score: %s' % rf_test_f1)\n",
    "\n",
    "# save the model to a file\n",
    "rf_model = './models/rf-model.sav'\n",
    "pickle.dump(rf, open(rf_model, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8263fe3-1fe8-49dc-8244-1c35921847ff",
   "metadata": {},
   "source": [
    "### **Neural Network Model (Multilayer Perceptron)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c96f56a-eb44-42e7-8fa3-010720804a74",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model performance for Training set\n",
      "- Brier: 0.007953599703537217\n",
      "- Accuracy: 0.9826130153999006\n",
      "- MCC: 0.9657385060506625\n",
      "- F1 score: 0.9825980815288354\n",
      "----------------------------------\n",
      "Model performance for Validation set\n",
      "- Brier: 0.008639082388457888\n",
      "- Accuracy: 0.9774436090225563\n",
      "- MCC: 0.9557256187490912\n",
      "- F1 score: 0.97741289393697\n",
      "----------------------------------\n",
      "Model performance for Test set\n",
      "- Brier: 0.007953599703537217\n",
      "- Accuracy: 0.9817922170653338\n",
      "- MCC: 0.9641132798826052\n",
      "- F1 score: 0.9817726198378026\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp = MLPClassifier(alpha=1, max_iter=1000)\n",
    "\n",
    "# Train model\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "mlp_train_pred = mlp.predict(X_train)\n",
    "mlp_val_pred = mlp.predict(X_val)\n",
    "mlp_test_pred = mlp.predict(X_test)\n",
    "\n",
    "# Make probabilities\n",
    "mlp_train_prob = mlp.predict_proba(X_train)[:, 1]\n",
    "mlp_val_prob = mlp.predict_proba(X_val)[:, 1]\n",
    "mlp_test_prob = mlp.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Training set performance\n",
    "mlp_train_brier = brier_score_loss(y_train, mlp_train_prob)\n",
    "mlp_train_accuracy = accuracy_score(y_train, mlp_train_pred)\n",
    "mlp_train_mcc = matthews_corrcoef(y_train, mlp_train_pred)\n",
    "mlp_train_f1 = f1_score(y_train, mlp_train_pred, average='weighted')\n",
    "\n",
    "# Validation set performance\n",
    "mlp_val_brier = brier_score_loss(y_val, mlp_val_prob)\n",
    "mlp_val_accuracy = accuracy_score(y_val, mlp_val_pred)\n",
    "mlp_val_mcc = matthews_corrcoef(y_val, mlp_val_pred)\n",
    "mlp_val_f1 = f1_score(y_val, mlp_val_pred, average='weighted')\n",
    "\n",
    "# Test set performance\n",
    "mlp_test_brier = brier_score_loss(y_test, mlp_test_prob)\n",
    "mlp_test_accuracy = accuracy_score(y_test, mlp_test_pred)\n",
    "mlp_test_mcc = matthews_corrcoef(y_test, mlp_test_pred)\n",
    "mlp_test_f1 = f1_score(y_test, mlp_test_pred, average='weighted')\n",
    "\n",
    "print('Model performance for Training set')\n",
    "print('- Brier: %s' % mlp_train_brier)\n",
    "print('- Accuracy: %s' % mlp_train_accuracy)\n",
    "print('- MCC: %s' % mlp_train_mcc)\n",
    "print('- F1 score: %s' % mlp_train_f1)\n",
    "print('----------------------------------')\n",
    "print('Model performance for Validation set')\n",
    "print('- Brier: %s' % mlp_val_brier)\n",
    "print('- Accuracy: %s' % mlp_val_accuracy)\n",
    "print('- MCC: %s' % mlp_val_mcc)\n",
    "print('- F1 score: %s' % mlp_val_f1)\n",
    "print('----------------------------------')\n",
    "print('Model performance for Test set')\n",
    "print('- Brier: %s' % mlp_train_brier)\n",
    "print('- Accuracy: %s' % mlp_test_accuracy)\n",
    "print('- MCC: %s' % mlp_test_mcc)\n",
    "print('- F1 score: %s' % mlp_test_f1)\n",
    "\n",
    "# save the model to a file\n",
    "mlp_model = './models/mlp-model.sav'\n",
    "pickle.dump(mlp, open(mlp_model, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0509fd44-6006-4661-a57c-64851ef2e6ae",
   "metadata": {},
   "source": [
    "### **Build Stacked Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e17de1-743d-4ca7-97a2-e20e3d9ad17a",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### **Custom function to split data based on provided year and number of years to go back**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2473f913-c8b4-4f8e-a620-110760b3cb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "build_stack_data(2023, 3); # Custom function to split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1f8f70ae-fbba-45f7-a4e6-55c47736555d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((16210, 19), (4053, 19))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rename Train, Val, Test data\n",
    "X_train_stack, X_test_stack = data_stack['X_train'], data_stack['X_test']\n",
    "y_train_stack, y_test_stack = data_stack['y_train'], data_stack['y_test']\n",
    "\n",
    "X_train_stack.shape, X_test_stack.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "94bf6110-0a82-4285-a9c8-067ee54a525a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16210, 19)\n",
      "(16210,)\n"
     ]
    }
   ],
   "source": [
    "# Check the shape of X_train_stack and y_train_stack\n",
    "print(X_train_stack.shape)\n",
    "print(y_train_stack.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d7289c95-1f95-40b7-bc30-aa5b037f1bd3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model performance for Training set\n",
      "- Brier: 1.981915733171282e-06\n",
      "- Accuracy: 1.0\n",
      "- MCC: 1.0\n",
      "- F1 score: 1.0\n",
      "----------------------------------\n",
      "Model performance for Test set\n",
      "- Brier: 1.0076158596093403e-05\n",
      "- Accuracy: 1.0\n",
      "- MCC: 1.0\n",
      "- F1 score: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Define estimators\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "estimator_list = [ ('svm', svm), ('knn', knn), ('gnb', gnb), ('rf', rf), ('mlp', mlp) ]\n",
    "\n",
    "# Build stack model\n",
    "model = StackingClassifier(estimators=estimator_list, final_estimator=LogisticRegression())\n",
    "\n",
    "# Train stacked model\n",
    "model.fit(X_train_stack, y_train_stack)\n",
    "\n",
    "# Make predictions\n",
    "stk_train_pred = model.predict(X_train_stack)\n",
    "stk_test_pred = model.predict(X_test_stack)\n",
    "\n",
    "# Make probabilities\n",
    "stk_train_prob = model.predict_proba(X_train_stack)[:, 1]\n",
    "stk_test_prob = model.predict_proba(X_test_stack)[:, 1]\n",
    "\n",
    "# Training set model performance\n",
    "stk_train_brier = brier_score_loss(y_train_stack, stk_train_prob)\n",
    "stk_train_accuracy = accuracy_score(y_train_stack, stk_train_pred) # Calculate Accuracy\n",
    "stk_train_mcc = matthews_corrcoef(y_train_stack, stk_train_pred) # Calculate MCC\n",
    "stk_train_f1 = f1_score(y_train_stack, stk_train_pred, average='weighted') # Calculate F1-score\n",
    "\n",
    "# Test set model performance\n",
    "stk_test_brier = brier_score_loss(y_test_stack, stk_test_prob)\n",
    "stk_test_accuracy = accuracy_score(y_test_stack, stk_test_pred) # Calculate Accuracy\n",
    "stk_test_mcc = matthews_corrcoef(y_test_stack, stk_test_pred) # Calculate MCC\n",
    "stk_test_f1 = f1_score(y_test_stack, stk_test_pred, average='weighted') # Calculate F1-score\n",
    "\n",
    "print('Model performance for Training set')\n",
    "print('- Brier: %s' % stk_train_brier)\n",
    "print('- Accuracy: %s' % stk_train_accuracy)\n",
    "print('- MCC: %s' % stk_train_mcc)\n",
    "print('- F1 score: %s' % stk_train_f1)\n",
    "print('----------------------------------')\n",
    "print('Model performance for Test set')\n",
    "print('- Brier: %s' % stk_test_brier)\n",
    "print('- Accuracy: %s' % stk_test_accuracy)\n",
    "print('- MCC: %s' % stk_test_mcc)\n",
    "print('- F1 score: %s' % stk_test_f1)\n",
    "\n",
    "# save the model to a file\n",
    "saved_model = './models/stacked-model.sav'\n",
    "pickle.dump(model, open(saved_model, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95a0360-b8a8-43fb-87fa-c895d75ffe79",
   "metadata": {},
   "source": [
    "### **Summarize Results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eab9fe11-d648-4209-810e-84607bbc6c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "brier_train_list = {'svm': svm_train_brier, 'knn': knn_train_brier, 'gnb': gnb_train_brier, \n",
    "                    'rf': rf_train_brier, 'mlp': mlp_train_brier, 'model': stk_train_brier}\n",
    "\n",
    "acc_train_list = {'svm': svm_train_accuracy, 'knn': knn_train_accuracy, 'gnb': gnb_train_accuracy, \n",
    "                  'rf': rf_train_accuracy, 'mlp': mlp_train_accuracy, 'model': stk_train_accuracy}\n",
    "\n",
    "mcc_train_list = {'svm': svm_train_mcc, 'knn': knn_train_mcc, 'gnb': gnb_train_mcc, \n",
    "                  'rf': rf_train_mcc, 'mlp': mlp_train_mcc, 'model': stk_train_mcc}\n",
    "\n",
    "f1_train_list = {'svm': svm_train_f1, 'knn': knn_train_f1, 'gnb': gnb_train_f1, \n",
    "                 'rf': rf_train_f1, 'mlp': mlp_train_f1, 'model': stk_train_f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f89e6018-feb3-4c76-be5b-af2947882451",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'svm': 2.7486432965980435e-05,\n",
       " 'knn': 0.028234475906607055,\n",
       " 'gnb': 0.02758298982492225,\n",
       " 'rf': 0.00032041728763040233,\n",
       " 'mlp': 0.007953599703537217,\n",
       " 'model': 1.981915733171282e-06}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brier_train_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "67d72628-c2b2-4fdd-840c-72311c6b408e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brier</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>MCC</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>svm</th>\n",
       "      <td>0.000027</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>0.028234</td>\n",
       "      <td>0.969051</td>\n",
       "      <td>0.938038</td>\n",
       "      <td>0.969049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gnb</th>\n",
       "      <td>0.027583</td>\n",
       "      <td>0.962047</td>\n",
       "      <td>0.924020</td>\n",
       "      <td>0.962047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>0.000320</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>0.007954</td>\n",
       "      <td>0.982613</td>\n",
       "      <td>0.965739</td>\n",
       "      <td>0.982598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <td>0.000002</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Brier  Accuracy       MCC        F1\n",
       "svm    0.000027  1.000000  1.000000  1.000000\n",
       "knn    0.028234  0.969051  0.938038  0.969049\n",
       "gnb    0.027583  0.962047  0.924020  0.962047\n",
       "rf     0.000320  1.000000  1.000000  1.000000\n",
       "mlp    0.007954  0.982613  0.965739  0.982598\n",
       "model  0.000002  1.000000  1.000000  1.000000"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Summarize results into a single dataframe\n",
    "brier_df = pd.DataFrame.from_dict(brier_train_list, orient='index', columns=['Brier'])\n",
    "acc_df = pd.DataFrame.from_dict(acc_train_list, orient='index', columns=['Accuracy'])\n",
    "mcc_df = pd.DataFrame.from_dict(mcc_train_list, orient='index', columns=['MCC'])\n",
    "f1_df = pd.DataFrame.from_dict(f1_train_list, orient='index', columns=['F1'])\n",
    "scores = pd.concat([brier_df, acc_df, mcc_df, f1_df], axis=1)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769fbbee-a2c5-4be9-98bf-942fd2f96679",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### **Artificial Neural Network Model {not looking too good}**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30063bc-1408-4f40-ac36-ce002cab3a14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "numerical_features = ['Seed', 'Site', 'MOV', 'FG2M', 'FG2A', 'FG3M', 'FG3A', 'FT1M', 'FT1A', \n",
    "                      'ORB', 'DRB', 'AST', 'TOVR', 'STL', 'BLK', 'PFL']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train[numerical_features])\n",
    "\n",
    "X_train_scaled.loc[:, numerical_features] = scaler.transform(X_train[numerical_features])\n",
    "X_val_scaled.loc[:, numerical_features] = scaler.transform(X_val[numerical_features])\n",
    "X_test_scaled.loc[:, numerical_features] = scaler.transform(X_test[numerical_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad21a38-3d97-4004-9308-21bee84b147f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, matthews_corrcoef, f1_score\n",
    "\n",
    "n_input = X_train.shape[1]\n",
    "\n",
    "ann = Sequential()\n",
    "ann.add(Dense(64, input_dim=n_input, activation='relu'))\n",
    "ann.add(Dense(32, activation='relu'))\n",
    "ann.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Build (compile) model\n",
    "ann.compile(loss='binary_crossentropy', optimizer='adam', \n",
    "            metrics=[brier_metric, 'binary_accuracy'])\n",
    "\n",
    "# Train model\n",
    "ann.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=32)\n",
    "\n",
    "# Make predictions\n",
    "ann_train_prob = ann.predict(X_train)\n",
    "ann_val_prob = ann.predict(X_val)\n",
    "ann_test_prob = ann.predict(X_test)\n",
    "\n",
    "# Convert probabilities to binary predictions\n",
    "ann_train_pred = np.round(ann_train_prob).astype(int)\n",
    "ann_val_pred = np.round(ann_val_prob).astype(int)\n",
    "ann_test_pred = np.round(ann_test_prob).astype(int)\n",
    "\n",
    "# Evaluate performance on training set\n",
    "ann_train_brier = K.eval(brier_metric(K.variable(y_train), K.variable(ann_train_prob)))\n",
    "ann_train_accuracy = accuracy_score(y_train, ann_train_pred)\n",
    "ann_train_mcc = matthews_corrcoef(y_train, ann_train_pred)\n",
    "ann_train_f1 = f1_score(y_train, ann_train_pred, average='weighted')\n",
    "\n",
    "# Evaluate performance on validation set\n",
    "ann_val_brier = K.eval(brier_metric(K.variable(y_val), K.variable(ann_val_prob)))\n",
    "ann_val_accuracy = accuracy_score(y_val, ann_val_pred)\n",
    "ann_val_mcc = matthews_corrcoef(y_val, ann_val_pred)\n",
    "ann_val_f1 = f1_score(y_val, ann_val_pred, average='weighted')\n",
    "\n",
    "# Evaluate performance on test set\n",
    "ann_test_brier = K.eval(brier_metric(K.variable(y_test), K.variable(ann_test_prob)))\n",
    "ann_test_accuracy = accuracy_score(y_test, ann_test_pred)\n",
    "ann_test_mcc = matthews_corrcoef(y_test, ann_test_pred)\n",
    "ann_test_f1 = f1_score(y_test, ann_test_pred, average='weighted')\n",
    "\n",
    "# Print results\n",
    "print('Model performance for Training set')\n",
    "print('- Brier: %s' % ann_train_brier)\n",
    "print('- Accuracy: %s' % ann_train_accuracy)\n",
    "print('- MCC: %s' % ann_train_mcc)\n",
    "print('- F1 score: %s' % ann_train_f1)\n",
    "print('----------------------------------')\n",
    "print('Model performance for Validation set')\n",
    "print('- Brier: %s' % ann_val_brier)\n",
    "print('- Accuracy: %s' % ann_val_accuracy)\n",
    "print('- MCC: %s' % ann_val_mcc)\n",
    "print('- F1 score: %s' % ann_val_f1)\n",
    "print('----------------------------------')\n",
    "print('Model performance for Test set')\n",
    "print('- Brier: %s' % ann_test_brier)\n",
    "print('- Accuracy: %s' % ann_test_accuracy)\n",
    "print('- MCC: %s' % ann_test_mcc)\n",
    "print('- F1 score: %s' % ann_test_f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py 3.09 (mlops)",
   "language": "python",
   "name": "mlops"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
